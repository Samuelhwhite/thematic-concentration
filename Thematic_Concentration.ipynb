{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Thematic Concentration",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXBFlxhDJ_sB",
        "colab_type": "text"
      },
      "source": [
        "To make full use of this notebook, then go to: File > Save a copy in Drive...\n",
        "\n",
        "This will allow you to keep a version in your own Drive that you can work from, and is recommended. You can then close out the original tab and work on the copied version.\n",
        "\n",
        "To find where it's been copied to go: File > Locate in Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEk1OeTdIsxv",
        "colab_type": "text"
      },
      "source": [
        "# Thematic Concentration.\n",
        "\n",
        "Aim: Use Wang et Liu to compute the h-point for each select Trump speeches found here: https://github.com/unendin/Trump_Campaign_Corpus/tree/master/text\n",
        "\n",
        "\n",
        "## Setup of workbook:\n",
        "\n",
        "### 1. Set up workbook for maximum speed (GPU)\n",
        "### 2. Import necessary modules\n",
        "### 3. Load in the data\n",
        "### 4. Data-preprocessing \n",
        "### 5. Part-of-Speech tagging and lemmatisation\n",
        "### 6. Compute h-point"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvOkR0PUJ05c",
        "colab_type": "text"
      },
      "source": [
        "### 1. Set up workbook:\n",
        "\n",
        "Google Colab generously gives you one GPU (graphics processing unit) to run computations on.\n",
        "A GPU is much quicker than a CPU, in that it can perform many more FLOPs (floating point operations [read \"calculations\"]) per second.\n",
        "\n",
        "To turn this feature on go to:\n",
        "Edit > Notebook Settings > Change the hardware accelerator to GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKjaCFf2IpZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf # Importing our first module (as below) but we need it \n",
        "                        # earlier to check whether we have the GPU running in the correct place!\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuCaRsJSKOhf",
        "colab_type": "text"
      },
      "source": [
        "### 2. Import necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JHz6mYAKM_7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "import collections\n",
        "!pip install spacy\n",
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K34RQjQCKZSL",
        "colab_type": "text"
      },
      "source": [
        "### 3. Load in the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaeRsCzbKbtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')   # Run this code and follow the instructions to mount your drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdl9GfjcKnww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_path = 'drive/My Drive/Colab Notebooks/Deep Learning/OSGD Workbooks/Katy Tur.txt' # replace this with where you've saved the text file in Drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDnEdViBK1Mn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_words(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        sentences = []\n",
        "        for line in f:\n",
        "          sentences.append(line.strip())\n",
        "    return sentences\n",
        "\n",
        "speech = read_words(text_path) # import single speech as a list of line-by-line text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcleD1b4Es2j",
        "colab_type": "text"
      },
      "source": [
        "### 4. Pre-process / clean the data\n",
        "\n",
        "Firstly we remove things not spoken by Donald (audience input etc.)\n",
        "\n",
        "Then we remove his introduction from each line he speaks, and turn the whole thing into lower case for ease of machine counting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TxnybP7LHmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_line(line):\n",
        "  if ('LAUGHTER' in line) or ('APPLAUSE' in line) or ('UNIDENTIFIED' in line) or ('INAUDIBLE' in line) or ('CROSSTALK' in line) or ('PROTESTERS' in line):\n",
        "    return False\n",
        "  elif line == '':\n",
        "    return False\n",
        "  else:\n",
        "    return True\n",
        "\n",
        "def remove_donald_punc(line):\n",
        "  line = line.replace('\\'', \"'\")\n",
        "  line = line.lower()\n",
        "  line = line.split(':')\n",
        "\n",
        "  if len(line) != 2: # extract only pieces of text that Donald says\n",
        "    line = line[0]\n",
        "  else:\n",
        "    if line[0] == 'donald trump':\n",
        "      line = line[1]\n",
        "    else:\n",
        "      line = ''\n",
        "\n",
        "  return line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io8Mw-vKQhbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned1 = [x for x in speech if clean_line(x) == True] # Remove audience participation etc. using function above\n",
        "cleaned2 = [remove_donald_punc(x) for x in cleaned1] # Remove all \"Donald Trump:\" intros\n",
        "cleaned2[0:10] # Print the result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCGq2pQtRODm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert back into one long sting of text:\n",
        "\n",
        "full_text = '' \n",
        "for i in range(len(cleaned2)):\n",
        "  full_text = full_text + ' ' + cleaned2[i]\n",
        "\n",
        "full_text # Show result."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JTLMbDRFZpZ",
        "colab_type": "text"
      },
      "source": [
        "### 5. Part-of-Speech (POS) tagging and Lemmatisation\n",
        "\n",
        "Here we label things as noun, verb, adjective etc. (POS tagging, so we can extract themamtic words later).\n",
        "\n",
        "We also form the lemma of everything for more accurate frequency counting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YGSAZSbTYVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\") # using the spacy Python package we imported.\n",
        "doc = nlp(full_text) # convert to spacy format."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i0wKQ-LCmaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lem_pos = {}\n",
        "tokens =[]\n",
        "lemmas = []\n",
        "pos = []\n",
        "\n",
        "for token in doc:\n",
        "  lem_pos[token.lemma_] = token.pos_ # extract each word along with it's lemma and pos. into a dictionary\n",
        "  tokens.append(token.text) # extract list of the ordered words\n",
        "  lemmas.append(token.lemma_) # extract the lemma that each word maps to\n",
        "  pos.append(token.pos_) # extract their pos\n",
        "\n",
        "lemmas2 = []\n",
        "for i in range(len(lemmas)):\n",
        "  if pos[i] != 'SPACE' and pos[i] != 'PUNCT' and pos[i] != 'SYM' and pos[i] != 'NUM': # remove punctuation etc. from being considered as part of the h-point\n",
        "    lemmas2.append(lemmas[i])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mza8VjZFGqoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(tokens))\n",
        "print(len(lemmas))\n",
        "print(len(pos))\n",
        "print(len(lemmas2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKMIPQNJIjzd",
        "colab_type": "text"
      },
      "source": [
        "### 6. Compute h-point\n",
        "\n",
        "- Gather frequencies\n",
        "- Rank frequencies\n",
        "- Calculate h-point\n",
        "- Return thematic words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD2HBd58HSfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def frequency_and_h(data): # function to return the frquency and rank of each lemma, as well as the h-point\n",
        "    \n",
        "  counter = collections.Counter(data)\n",
        "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0])) # count the frequency of each lemma and return them in sorted order.\n",
        "\n",
        "  lem_freq_h = []\n",
        "  result = True # use booleans to stop the conditions applying beyond the h-point\n",
        "  cut = False\n",
        "  bigger = False\n",
        "\n",
        "  for i in range(len(count_pairs)-1):\n",
        "    ri = count_pairs[i]\n",
        "    item = (ri[0], ri[1], i+1) # put the rank next to each lemma\n",
        "    lem_freq_h.append(item)\n",
        "\n",
        "    if ri[1] == i+1 and result == True: # See if we have a frequency matching it's rank at any point.\n",
        "      cut = i+1\n",
        "      result = False\n",
        "\n",
        "    if ri[1] < i+1 and bigger == False: # if not, calculate the h point between the frequencies that straggle the point\n",
        "      ri_1 = count_pairs[i-1]\n",
        "      cut = (ri_1[1]*(i+1) - (i)*ri[1]) / (1 + ri_1[1] - ri[1]) # h-point formula from the paper\n",
        "      bigger = True\n",
        "\n",
        "  return lem_freq_h, cut"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv7cVujKW6FR",
        "colab_type": "text"
      },
      "source": [
        "Now just to return thematic words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9QB38XPKhXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def return_thematic(lemmas, lem_pos):\n",
        "\n",
        "  count_pairs, cut = frequency_and_h(lemmas) # using the function above to extract the h-point and the ordered lemmas.\n",
        "  print(\"h-point is: {}\".format(cut)) # print h-point\n",
        "  print('')\n",
        "\n",
        "  thematic = [] # prepare list to store thematic values for calculation of TC value\n",
        "  print('Thematic words (nouns and adjectives): ')\n",
        "  for i in range(round(cut)):\n",
        "    lemma = count_pairs[i][0] # return those that are nouns\n",
        "    if lem_pos[lemma] == 'NOUN' or lem_pos[lemma] == 'ADJ':\n",
        "      print(lemma)\n",
        "      thematic.append(count_pairs[i])\n",
        "  \n",
        "  print('')\n",
        "  multiplier = 2/(cut*(cut-1)*count_pairs[0][1])\n",
        "  calc = 0\n",
        "  for ii in range(len(thematic)):\n",
        "    val = (cut - thematic[ii][2])*thematic[ii][1]\n",
        "    calc += val\n",
        "  TC = multiplier*calc\n",
        "  print('Thematic concentration is: {}'.format(TC))\n",
        "\n",
        "\n",
        "  print('')\n",
        "  print('Words above h-point: (Word, Frequency, Rank): ')\n",
        "  for i in range(round(cut)):\n",
        "    print(count_pairs[i]) # print words above the h point.\n",
        "\n",
        "  print('')\n",
        "  print('All nouns & adjectives by frequency/rank:')\n",
        "\n",
        "  for i in range(len(count_pairs)):\n",
        "    lemma = count_pairs[i][0] # return those that are nouns\n",
        "    if lem_pos[lemma] == 'NOUN' or lem_pos[lemma] == 'ADJ':\n",
        "      print(count_pairs[i])\n",
        "\n",
        "  print('')\n",
        "  print('Top 100 words by frequency:')\n",
        "  for i in range(100):\n",
        "    print(count_pairs[i])\n",
        "\n",
        "  print('')\n",
        "\n",
        "  return 'Done'\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7fWHfDFKpsb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "return_thematic(lemmas2, lem_pos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDMcUMusQ1ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
